{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25196a28-405f-4cb5-b8e2-c43b5fb9f5b0",
   "metadata": {},
   "source": [
    "# TP 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34a72776-f183-484e-ac14-811c3cc43747",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a281c3f9-96d1-4e5d-8aee-2d649ba85374",
   "metadata": {},
   "source": [
    "### Exercise 1: Explore the FrozenLake environment from OpenAI Gym."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce728b20-ca19-4381-aae8-4255123bc81d",
   "metadata": {},
   "source": [
    "Charger l’environnement FrozenLake-v1 de OpenAI Gym "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "977a0b72-3b84-42b2-8406-700c3277ebe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "env =gym.make(\"FrozenLake-v1\",is_slippery=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2047d4-dfca-442a-86c3-ff7ec9dd859d",
   "metadata": {},
   "source": [
    "Afficher les informations de l'espace d'états et d'actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00f76685-ec64-4393-a7e5-0eccc9b8a1d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Espace d'états: 16\n",
      "Espace d'actions: 4\n"
     ]
    }
   ],
   "source": [
    "print(\"Espace d'états:\", env.observation_space.n)\n",
    "print(\"Espace d'actions:\", env.action_space.n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3d8886-7472-4971-b42a-ccfddeb8518e",
   "metadata": {},
   "source": [
    "Exécuter une boucle avec des actions aléatoires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11a5ff08-be50-4606-bd58-fc39a133c6a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Épisode 1, Récompense totale: 0.0\n",
      "Épisode 2, Récompense totale: 0.0\n",
      "Épisode 3, Récompense totale: 0.0\n",
      "Épisode 4, Récompense totale: 0.0\n",
      "Épisode 5, Récompense totale: 0.0\n",
      "Épisode 6, Récompense totale: 0.0\n",
      "Épisode 7, Récompense totale: 0.0\n",
      "Épisode 8, Récompense totale: 0.0\n",
      "Épisode 9, Récompense totale: 0.0\n",
      "Épisode 10, Récompense totale: 0.0\n"
     ]
    }
   ],
   "source": [
    "num_episodes = 10\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    \n",
    "    while not done:\n",
    "        action = env.action_space.sample()  # Action aléatoire\n",
    "        next_state, reward, done, info,_= env.step(action)\n",
    "        total_reward += reward\n",
    "        state = next_state\n",
    "    print(f\"Épisode {episode + 1}, Récompense totale: {total_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038cec30-81f0-4e50-b364-05f2ac157c42",
   "metadata": {},
   "source": [
    "### Exercice 2 : Implémentation de la méthode d'initialisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea287240-08d2-4abd-8430-bda859eaa8a5",
   "metadata": {},
   "source": [
    "Initialisation de la Q-Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f588fea0-7c5b-41e4-aff5-a87c11a4ed27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-Table initialisée :\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "num_states = env.observation_space.n\n",
    "num_actions = env.action_space.n\n",
    "q_table = np.zeros((num_states, num_actions))\n",
    "\n",
    "print(\"Q-Table initialisée :\")\n",
    "print(q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259583c1-5588-4754-8f84-5649bb1c55e2",
   "metadata": {},
   "source": [
    "### Exercice 3 : Implémentation du Q-Learning avec Mise à Jour"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4eec597-d4b6-4b2c-9f18-bf42128d1392",
   "metadata": {},
   "source": [
    "Définir les hyperparamètres : taux d'apprentissage (alpha), facteur de discount (gamma), epsilon pour l'exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "59e3d47a-1fac-46c4-8c99-fc26c33c1ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.1           \n",
    "gamma = 0.99          \n",
    "epsilon = 1.0        \n",
    "epsilon_decay = 0.995\n",
    "min_epsilon = 0.01\n",
    "num_episodes = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "af6b4f72-cac8-47b9-bd90-754d634ed3a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-Table après apprentissage :\n",
      "[[0.56304039 0.52878801 0.52780531 0.52187655]\n",
      " [0.34042933 0.31639782 0.32884872 0.50660591]\n",
      " [0.38974347 0.4174704  0.38195381 0.48001746]\n",
      " [0.26214024 0.31350918 0.22212528 0.46258896]\n",
      " [0.59552922 0.37119849 0.3969272  0.39191666]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.34254044 0.13917688 0.18413976 0.1362371 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.47141058 0.49137828 0.30910645 0.63788414]\n",
      " [0.45401603 0.706711   0.42827704 0.30567809]\n",
      " [0.68146431 0.41819101 0.42873238 0.3946734 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.45123549 0.32109285 0.82601651 0.48856151]\n",
      " [0.72211168 0.88860642 0.76351244 0.785471  ]\n",
      " [0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Entraînement de l’agent\n",
    "episode = 0\n",
    "while episode < num_episodes:\n",
    "    state, info = env.reset()\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        # Choix de l’action : exploration ou exploitation\n",
    "        if np.random.uniform(0, 1) < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = np.argmax(q_table[state, :])\n",
    "\n",
    "        # Exécution de l’action\n",
    "        new_state, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "        # Mise à jour de la Q-Table\n",
    "        old_value = q_table[state, action]\n",
    "        next_max = np.max(q_table[new_state, :])\n",
    "        new_value = old_value + alpha * (reward + gamma * next_max - old_value)\n",
    "        q_table[state, action] = new_value\n",
    "\n",
    "        state = new_state\n",
    "\n",
    "    # Mise à jour du taux d’exploration avec une boucle while\n",
    "    while epsilon > min_epsilon:\n",
    "        epsilon = epsilon * epsilon_decay\n",
    "        break  # L'epsilon est réduit une seule fois par épisode\n",
    "\n",
    "    episode += 1\n",
    "\n",
    "# Affichage final de la Q-Table\n",
    "print(\"Q-Table après apprentissage :\")\n",
    "print(q_table)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df00253-e3e5-4b5f-92b4-86dba9fde7a1",
   "metadata": {},
   "source": [
    "### Exercice 4 : Évaluation des Performances de l'Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "968c6fac-8345-41b7-aaf3-e6caad56e901",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taux de réussite: 83.0%\n"
     ]
    }
   ],
   "source": [
    "num_test_episodes = 100\n",
    "successes = 0\n",
    "\n",
    "for _ in range(num_test_episodes):\n",
    "    state = env.reset()[0]\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        action = np.argmax(q_table[state])  # Toujours l'action optimale\n",
    "        next_state, reward, done, truncated, info = env.step(action)\n",
    "        \n",
    "        if done and reward == 1:\n",
    "            successes += 1\n",
    "            \n",
    "        state = next_state\n",
    "\n",
    "success_rate = (successes / num_test_episodes) * 100\n",
    "print(f\"Taux de réussite: {success_rate}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be07b84e-3448-405d-a4f4-13f00e2b135d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

# Reinforcement Learning 

![Python](https://img.shields.io/badge/Python-3.8%2B-blue)

Le Reinforcement Learning (Apprentissage par Renforcement) est une branche du Machine Learning o√π un agent apprend √† interagir avec un environnement pour maximiser une r√©compense cumul√©e. Ce d√©p√¥t explore les concepts cl√©s √† travers des TP pratiques utilisant OpenAI Gym, Q-Learning, SARSA et PPO.

Ce d√©p√¥t contient les travaux pratiques de Machine Learning II sur l'apprentissage par renforcement, r√©alis√©s dans le cadre du cours √† l'√âcole Nationale de l'Intelligence Artificielle et du Digital.

## üìã Table des mati√®res
- [TP1: D√©couverte d'OpenAI Gym](#tp1-d√©couverte-dopenai-gym)
- [TP2: Algorithmes de Base (Q-Learning/SARSA)](#tp2-algorithmes-de-base-q-learningsarsa)
- [TP3 - Optimisation des Feux de Circulation par Reinforcement Learning](#tp3---optimisation-des-feux-de-circulation-par-reinforcement-learning)
- [TP4 - Proximal Policy Optimization (PPO)](#tp4---proximal-policy-optimization-ppo)
- [Guide d'installation](#guide-dinstallation)

## TP1: D√©couverte d'OpenAI Gym

### üéØ Objectif
Prendre en main les environnements Gym et les concepts de base du Reinforcement Learning (RL).

### üìñ D√©finition et R√¥le d'OpenAI Gym
**OpenAI Gym** est une bo√Æte √† outils standardis√©e pour le d√©veloppement et la comparaison d'algorithmes d'apprentissage par renforcement. Son r√¥le principal comprend :

1. **Standardisation** :
   - Fournit une interface commune pour tous les environnements (m√©thodes `reset()`, `step()`)
   - Permet des comparaisons √©quitables entre algorithmes

2. **Biblioth√®que d'environnements** :
   - Environnements classiques (CartPole, MountainCar)
   - Environnements Atari (Jeux vid√©o)
   - Environnements 2D/3D de physique (MuJoCo)

3. **Outils d'√©valuation** :
   - M√©triques standardis√©es (r√©compense cumul√©e, dur√©e des √©pisodes)
   - Capacit√© d'enregistrement des r√©sultats

4. **Flexibilit√©** :
   - Prise en charge de la cr√©ation d'environnements personnalis√©s
   - Compatibilit√© avec PyTorch et TensorFlow

### Fonctionnement de Base
Le flux typique d'interaction avec Gym suit ce sch√©ma :

```mermaid
stateDiagram-v2
    [*] --> Initialisation
    Initialisation --> Boucle: reset()
    Boucle --> Terminaison
    Boucle --> Boucle: step(action)
```

## Impl√©mentation Gymnasium - CartPole

### 1. Importation et Configuration

```python
import gymnasium as gym
env = gym.make("CartPole-v1", render_mode="human")
```

**CartPole-v1**: Environnement classique de contr√¥le
**render_mode="human"** : Active la visualisation



```python
import gymnasium as gym
```

### 2. Cr√©ation de l'environnement
```python
env = gym.make("CartPole-v1", render_mode="human")  # Mode "human" pour la visualisation
```
### 3. Initialisation
```python
observation, info = env.reset()  # R√©initialise l'environnement et retourne l'√©tat initial
print("√âtat initial:", observation)
```

### 4. Exploration des propri√©t√©s
```python
print("\nPropri√©t√©s de l'environnement:")
print("- Espace d'actions:", env.action_space)       # Discrete(2) ‚Üí 0=gauche, 1=droite
print("- Espace d'observations:", env.observation_space)  # Box(4,) ‚Üí [position, vitesse, angle, vitesse angulaire]
```

### 5. Ex√©cution d'√©pisodes al√©atoires
```python
for episode in range(3):  # 3 √©pisodes de d√©monstration
    observation = env.reset()[0]
    done = False
    total_reward = 0
    
    while not done:
        # A. Rendu visuel
        env.render()  
        
        # B. S√©lection d'action al√©atoire
        action = env.action_space.sample()  
        
        # C. Ex√©cution de l'action
        observation, reward, terminated, truncated, info = env.step(action)
        
        # D. Gestion de la terminaison
        done = terminated or truncated
        total_reward += reward
        
    print(f"\n√âpisode {episode+1}:")
    print("- R√©compense totale:", total_reward)
    print("- Dernier √©tat:", observation)
```

### 6. Fermeture propre
```python
env.close()
````


[![Regarder la vid√©o](22.png)](1.mp4)  

üëâ *Cliquez sur l'image ci-dessus pour voir la vid√©o !*


## TP2: Algorithmes de Base (Q-Learning/SARSA)

### üéØ Objectifs
1. Impl√©menter les algorithmes fondamentaux du RL tabulaire
2. Comparer Q-Learning (off-policy) et SARSA (on-policy)
3. Analyser les politiques apprises
  

### üìö Th√©orie Cl√©
| Algorithme   | Type       | Mise √† jour                                                                 | Exploration |
|--------------|------------|-----------------------------------------------------------------------------|-------------|
| **Q-Learning** | Off-policy | `Q(s,a) ‚Üê Q(s,a) + Œ±[r + Œ≥ max‚Çê Q(s',a') - Q(s,a)]`                        | Œµ-greedy    |
| **SARSA**      | On-policy  | `Q(s,a) ‚Üê Q(s,a) + Œ±[r + Œ≥ Q(s',a') - Q(s,a)]`                             | Œµ-greedy    |

---

## 1. Q-Learning (Off-Policy)

### Principe Fondamental
**Apprend la politique optimale** ind√©pendamment des actions effectivement choisies pendant l'apprentissage.

#### Pseudocode
```python
Initialiser Q-table arbitrairement
Pour chaque √©pisode :
    s = √©tat initial
    Tant que non terminal :
        Choisir a via Œµ-greedy(Q,s)
        Ex√©cuter a, observer r, s'
        Q(s,a) += Œ±[r + Œ≥ max‚Çê Q(s',a') - Q(s,a)]
        s = s'

```

## Impl√©mentation Q-Learning

### 1. Initialisation
```python
import numpy as np

# Hyperparam√®tres
alpha = 0.1  # Taux d'apprentissage
gamma = 0.99  # Facteur de discount
epsilon = 0.1  # Exploration
episodes = 1000

# Q-table (√©tats √ó actions)
q_table = np.zeros((state_size, action_size))
```
### 2. Boucle d'Apprentissage
```python

for episode in range(episodes):
    state = env.reset()[0]
    done = False
    
    while not done:
        # S√©lection d'action Œµ-greedy
        if np.random.random() < epsilon:
            action = env.action_space.sample()
        else:
            action = np.argmax(q_table[state])
        
        # Ex√©cution
        next_state, reward, terminated, truncated, _ = env.step(action)
        done = terminated or truncated
        
        # Mise √† jour Q-learning
        best_next = np.max(q_table[next_state])
        q_table[state, action] += alpha * (reward + gamma * best_next - q_table[state, action])
        
        state = next_state
```
## Impl√©mentation SARSA

### 1. Initialisation (identique √† Q-learning)
### 2. Boucle d'Apprentissage
```python
for episode in range(episodes):
    state = env.reset()[0]
    done = False
    
    # S√©lection initiale d'action
    if np.random.random() < epsilon:
        action = env.action_space.sample()
    else:
        action = np.argmax(q_table[state])
    
    while not done:
        # Ex√©cution
        next_state, reward, terminated, truncated, _ = env.step(action)
        done = terminated or truncated
        
        # S√©lection de la prochaine action (SARSA)
        if np.random.random() < epsilon:
            next_action = env.action_space.sample()
        else:
            next_action = np.argmax(q_table[next_state])
        
        # Mise √† jour SARSA
        q_table[state, action] += alpha * (reward + gamma * q_table[next_state, next_action] - q_table[state, action])
        
        state, action = next_state, next_action
```

## TP3 - Optimisation des Feux de Circulation par Reinforcement Learning

### üìö Objectifs
- **Comprendre** un environnement de contr√¥le urbain complexe
- **Adapter** les algorithmes de RL √† un probl√®me concret
- **Analyser** l'impact des politiques apprises sur le trafic
## üåê Environnement `TrafficEnvironment`

### üèó Structure
```python
class TrafficEnvironment:
    def __init__(self):
        self.state = np.random.randint(0, 10, size=4)  # [Nord, Sud, Est, Ouest]
        self.current_light = 0  # 0: Vert NS, 1: Vert EW
```

### Caract√©ristiques Techniques

| Composant       | Type         | Valeurs                     | Description                     |
|-----------------|--------------|-----------------------------|---------------------------------|
| **√âtat**        | `Box(4,)`    | [0-10] pour chaque direction| Nombre de v√©hicules en attente (Nord, Sud, Est, Ouest) |
| **Action**      | `Discrete(2)`| 0 ou 1                      | 0: Maintenir les feux actuels, 1: Changer les feux |
| **R√©compense**  | `float`      | ‚â•0                         | Nombre de v√©hicules ayant travers√© l'intersection |

### Dynamique du Syst√®me

1. **Passage des v√©hicules** :
   - Jusqu'√† 4 v√©hicules peuvent passer par cycle sur la voie verte
   - Passage stochastique : `randint(1,5)` v√©hicules passent effectivement

2. **Arriv√©e de nouveaux v√©hicules** :
   ```python
   new_cars = np.random.randint(0, 3, size=4)  # Ajoute 0-2 v√©hicules par direction

# Comparaison des performances de Q-Learning et SARSA

## R√©sultats des algorithmes

L'analyse comparative entre Q-Learning et SARSA r√©v√®le les performances suivantes :

| Algorithme   | R√©compense Cumulative Moyenne |
|--------------|-------------------------------|
| Q-Learning   | 327.77                        |
| SARSA        | 327.06                        |

## Visualisation des performances

![Comparaison Q-Learning vs SARSA](visualisation.png)

### Observations :
- **Q-Learning** atteint une r√©compense cumulative plus √©lev√©e (327.77) que **SARSA** (327.06 ) apr√®s 1000 √©pisodes.
- Les deux algorithmes montrent une progression significative au cours des 200 premiers √©pisodes.

# TP4 - Proximal Policy Optimization (PPO)

## Objectifs
- Impl√©menter l'algorithme PPO pour optimiser une politique d'agent RL.
- √âvaluer les performances sur un environnement sp√©cifique avec des √©pisodes d√©finis.
- Analyser l'impact du clipping et des avantages (_advantages_) sur la stabilit√© de l'apprentissage.

---

## Th√©orie Cl√© : Fonction de Perte PPO
La fonction de perte de PPO inclut un terme de clipping pour √©viter des mises √† jour trop grandes :  


$$L(\theta) = \mathbb{E}_t \left[ \min \left( r_t(\theta) A_t, \text{clip}(r_t(\theta), 1 - \epsilon, 1 + \epsilon) A_t \right) \right]$$


O√π :
- $$r_t(\theta) = \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_\text{old}}(a_t|s_t)}$$ (ratio des politiques)
- $$A_t$$ : Estimation de l'avantage
- $$\epsilon$$ : Param√®tre de clipping (ex: 0.2)

## Guide d'installation

1. **Environnements de base** :
```bash
pip install --upgrade gymnasium pygame numpy
```
```bash
# Cloner le d√©p√¥t
git clone https://github.com/Maryam-ELALAMI/reinforcement-learning.git
cd reinforcement-learning
